{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 8. Introduction to Feedforward Neural Networks**\n",
    "### **Summary of Feed-Forward Neural Networks**\n",
    "\n",
    "## **1. Introduction to Feed-Forward Neural Networks**\n",
    "- **Goal**: Understand feed-forward neural networks (FFNNs) as models and learn them from data.\n",
    "- Neural networks are extensions of nonlinear predictors, but they **optimize the feature representation** for a specific task&#8203;:contentReference[oaicite:0]{index=0}.\n",
    "- **Key Challenge**: Jointly learning feature representation and model parameters (chicken-and-egg problem).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Biological Inspiration and Artificial Abstraction**\n",
    "- **Real Neural Networks**:\n",
    "   - Composed of neurons that aggregate input signals through dendrites and propagate them through axons.\n",
    "- **Artificial Neurons**:\n",
    "   - Simplified as linear classifiers with input coordinates weighted by parameters.\n",
    "   - Aggregate input: $ z = \\sum_{i=1}^d x_i w_i + w_0 $.\n",
    "   - Output: Nonlinear transformation $ f(z) $ using **activation functions**.\n",
    "\n",
    "### **Common Activation Functions**:\n",
    "1. **Linear**: $ f(z) = z $ â€“ used for output layers.\n",
    "2. **ReLU** (Rectified Linear Unit): $ f(z) = \\max(0, z) $.\n",
    "3. **Tanh**: Smooth nonlinearity squashing values between -1 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Network Architecture**\n",
    "- **Layer Structure**:\n",
    "   - **Input Layer**: Holds input coordinates.\n",
    "   - **Hidden Layers**: Intermediate transformations of input data.\n",
    "   - **Output Layer**: Final prediction.\n",
    "- **Width**: Number of units in a layer.  \n",
    "- **Depth**: Number of layers in the network.\n",
    "\n",
    "- **Computation**:\n",
    "   - Each unit takes weighted inputs from the previous layer and passes the aggregate input through an activation function.  \n",
    "   - Output layers compute a final weighted combination of hidden unit activations.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Role of Hidden Layers**\n",
    "- Hidden layers transform input data into a representation that simplifies the final classification task.\n",
    "- **Hidden Units**:\n",
    "   - Function like linear classifiers, creating **decision boundaries** in input space.\n",
    "   - Nonlinear activation functions allow for richer transformations of data.\n",
    "\n",
    "### **Example**:\n",
    "- A linearly inseparable problem in $ x_1, x_2 $ can become linearly separable in the hidden layer's transformed space.\n",
    "\n",
    "### **Visualization**:\n",
    "- **Linear Units**: Maintain linear mappings, limiting transformation power.\n",
    "- **Nonlinear Units** (e.g., Tanh, ReLU): Enable more expressive mappings, improving separability in hidden layer activations.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Power of Depth and Redundancy**\n",
    "- Deep architectures combine layers to perform increasingly abstract computations.\n",
    "- Redundancy in hidden units (e.g., expanding dimensions) helps learn better representations and simplifies optimization.\n",
    "\n",
    "### **Why Neural Networks Work**:\n",
    "1. **Data Availability**: Large datasets allow learning complex models.\n",
    "2. **Computation**: Modern hardware (GPUs, TPUs) efficiently handles parallel computations.\n",
    "3. **Optimization**: Stochastic gradient descent (SGD) is effective for learning large models.\n",
    "4. **Modularity**: Neural networks serve as flexible computational components for diverse tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary of Key Points**\n",
    "- Feed-forward neural networks are composed of **input layers, hidden layers**, and **output layers**.  \n",
    "- **Hidden layers** play a critical role by transforming input data into a representation that is easier for the output layer to classify.  \n",
    "- **Activation functions** like ReLU and Tanh enable nonlinearity, allowing networks to model complex data.  \n",
    "- The depth of a network (number of layers) and redundancy in hidden units help achieve better performance.  \n",
    "- Despite their complexity, neural networks can be trained efficiently with simple optimization techniques like **stochastic gradient descent (SGD)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 9. Feedforward Neural Networks, Back Propagation, and Stochastic Gradient Descent (SGD) 4 of 5 possible points**\n",
    "\n",
    "# **Summary of Feed-Forward Neural Networks and Learning**\n",
    "\n",
    "## **1. Learning Feed-Forward Neural Networks**\n",
    "- **Goal**: Train a feed-forward neural network to learn a **mapping** from input $ x $ to output $ y $.  \n",
    "- Neural networks optimize **feature representation** and **parameters** simultaneously to minimize prediction error.\n",
    "- **Key Challenge**: Compute gradients of the loss with respect to the parameters efficiently using **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Stochastic Gradient Descent (SGD)**\n",
    "- **SGD Process**:\n",
    "   1. Compute the **loss**: Measures the difference between predicted output and target $ y $.  \n",
    "   2. Compute the **gradient**: Derivative of the loss with respect to each parameter.  \n",
    "   3. Update parameters:  \n",
    "      $\n",
    "      \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla L(\\theta_{\\text{old}})\n",
    "      $\n",
    "      - $ \\eta $: Learning rate.\n",
    "\n",
    "### **Backpropagation**:\n",
    "- **Purpose**: Efficiently compute gradients for all layers of the network.  \n",
    "- **Steps**:\n",
    "   1. Compute the **forward pass**: Evaluate activations layer by layer.\n",
    "   2. Compute the **loss** at the output.\n",
    "   3. Propagate the gradients **backward** using chain rule:\n",
    "      - Gradients are calculated layer by layer and propagated to earlier layers.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Example of Backpropagation**\n",
    "- **Simple Network**: One unit per layer, input $ x $, output $ f_L $, and loss $ L $.\n",
    "   - **Forward Pass**: $ z_1 = x \\cdot w_1 $, $ f_1 = \\tanh(z_1) $, etc.\n",
    "   - **Gradient Computation**:\n",
    "      1. Compute the loss gradient with respect to final layer output:\n",
    "         $\n",
    "         \\frac{\\partial L}{\\partial f_L} = - (y - f_L) \\quad \\text{(squared loss example)}\n",
    "         $\n",
    "      2. Propagate gradients backward through the layers using Jacobians:\n",
    "         $\n",
    "         \\frac{\\partial L}{\\partial f_1} = \\frac{\\partial L}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial f_1}\n",
    "         $\n",
    "- Issues in deep networks:\n",
    "   - **Vanishing Gradients**: Small derivatives cause gradients to shrink.  \n",
    "   - **Exploding Gradients**: Large derivatives lead to unstable updates.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Overcapacity and Learning Representations**\n",
    "- **Overcapacity**: Providing more hidden units than necessary can facilitate learning and optimization.  \n",
    "- Example:\n",
    "   - Networks with extra hidden units can find better representations and achieve lower loss.  \n",
    "   - Some units may not be \"useful,\" but redundancy helps optimization.  \n",
    "\n",
    "### **Visualization**:\n",
    "- Hidden units evolve during training to provide decision boundaries that simplify the classification task.\n",
    "- Adding random initializations or more hidden units can smooth the decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Challenges in Training Deep Networks**\n",
    "- **Gradient Vanishing/Explosion**:\n",
    "   - Gradients become very small or very large as they propagate through layers.  \n",
    "   - Solutions:\n",
    "     - Use **ReLU** activation functions.\n",
    "     - Introduce normalization techniques like **Batch Normalization**.\n",
    "\n",
    "- **Initialization**:\n",
    "   - Proper initialization of weights (including offsets) avoids artifacts like phantom boundaries.\n",
    "\n",
    "- **Local Optima**:\n",
    "   - Stochastic gradient descent typically finds **locally optimal solutions**, which are often sufficient for good performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary of Key Points**\n",
    "- Feed-forward neural networks are trained using **stochastic gradient descent** and **backpropagation**.\n",
    "- **Backpropagation** propagates the loss gradient efficiently through layers to update parameters.\n",
    "- **Overcapacity** in networks (adding more hidden units) improves optimization and facilitates learning.\n",
    "- Challenges like gradient vanishing/explosion and proper initialization must be addressed for successful training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 10. Recurrent Neural Networks 1** \n",
    "\n",
    "# **Summary of Recurrent Neural Networks (RNNs) and Sequence Modeling**\n",
    "\n",
    "## **1. Introduction to Sequence Modeling**\n",
    "- **Goal**: Predict properties of sequences, such as the next word in a sentence, sentiment, or translation.\n",
    "- Unlike feed-forward networks, recurrent neural networks (RNNs) model sequences more flexibly by retaining history information in a state vector.\n",
    "\n",
    "### **Challenges with Fixed-Length Representations**:\n",
    "- Fixed-length history may miss important information from earlier parts of the sequence.\n",
    "- A flexible mechanism is needed to encode variable-length sequences into meaningful vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Recurrent Neural Networks (RNNs)**\n",
    "- **RNN Concept**:\n",
    "  - RNNs process sequences by applying the same transformation repeatedly at each step.\n",
    "  - They update a **state vector** $ s_t $ based on the previous state $ s_{t-1} $ and new input $ x_t $:  \n",
    "    $\n",
    "    s_t = \\tanh(W_{ss} s_{t-1} + W_{sx} x_t)\n",
    "    $ \n",
    "  - Parameters $ W_{ss} $ and $ W_{sx} $ are learned to optimize the task.\n",
    "\n",
    "- **Key Properties**:\n",
    "  1. **Retain State**: State $ s_t $ summarizes the sequence seen so far.\n",
    "  2. **Parameter Sharing**: The same parameters are applied across all time steps, reducing complexity.\n",
    "  3. **Variable-Length Sequences**: RNNs adapt to sequences of any length.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Applications of RNNs**\n",
    "RNNs encode sequences into vectors for various prediction tasks:\n",
    "1. **Next Word Prediction**: Predict the next word in a sentence.\n",
    "2. **Sequence-Level Tasks**: Predict properties like sentiment or classify entire sequences.\n",
    "3. **Machine Translation**: Encode a sentence into a vector and decode it into another language.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Encoding Sequences into Vectors**\n",
    "- RNNs turn sequences into feature vectors **piecemeal**:\n",
    "   1. Start with an initial state $ s_0 = 0 $.\n",
    "   2. At each step $ t $, combine the previous state $ s_{t-1} $ with the current word vector $ x_t $.\n",
    "   3. Apply a **nonlinear transformation** (e.g., $ \\tanh $) to produce the new state $ s_t $.\n",
    "\n",
    "- **Example**:\n",
    "   - $ s_1 $ summarizes the first word.\n",
    "   - $ s_2 $ combines $ s_1 $ and the second word, continuing sequentially until the entire sequence is represented.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Gated Architectures for RNNs**\n",
    "- **Challenge**: Vanilla RNNs suffer from **vanishing** or **exploding gradients** when processing long sequences.\n",
    "- **Solution**: Use **gating mechanisms** to control state updates:\n",
    "   - **Gated Recurrent Unit (GRU)**: Adds a gating network to retain or overwrite information.\n",
    "   - **LSTM (Long Short-Term Memory)**:\n",
    "     - Adds gates to **forget**, **input**, and **output** information:\n",
    "       - **Forget Gate**: Controls which parts of the previous state to discard.\n",
    "       - **Input Gate**: Controls how much new information to add.\n",
    "       - **Output Gate**: Controls what part of the memory to reveal as visible state.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Training RNNs**\n",
    "- RNNs are trained using **backpropagation through time (BPTT)**:\n",
    "   - Compute the loss at the output.\n",
    "   - Backpropagate gradients **through time steps** to update parameters.  \n",
    "- **Issues**:\n",
    "   - **Vanishing Gradients**: Small gradients diminish, making it hard to train long sequences.\n",
    "   - **Exploding Gradients**: Large gradients cause instability.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Encoding vs. Decoding**\n",
    "- **Encoding**: Turn a sequence into a meaningful vector (e.g., for sentiment analysis).\n",
    "- **Decoding**: Use the encoded vector to generate predictions, including sequences (e.g., translation).\n",
    "\n",
    "### **Power of Encoded Vectors**:\n",
    "- Encoded representations allow objects like sentences, images, or events to be mapped into the same space.\n",
    "- This enables tasks like translating sentences to images, relating disparate objects, and flexible sequence modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Key Takeaways**\n",
    "- RNNs enable flexible modeling of sequences by maintaining a **state vector** that evolves over time.\n",
    "- **Gated architectures** (GRU, LSTM) improve training stability and address long-sequence challenges.\n",
    "- RNNs encode sequences into vectors that can be used for various tasks, from next-word prediction to machine translation.\n",
    "- Backpropagation through time (BPTT) is used for training, but gradient issues must be managed carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 11. Recurrent Neural Networks 2** \n",
    "\n",
    "# **Summary of Sequence Generation and Recurrent Neural Networks**\n",
    "\n",
    "## **1. Introduction to Sequence Generation**\n",
    "- **Goal**: Use recurrent neural networks (RNNs) to generate sequences, such as sentences or character streams.\n",
    "- **Key Idea**:\n",
    "   - Predict the next word in a sequence based on previous words.\n",
    "   - Translate inputs (e.g., words, images) into meaningful vector representations and decode them back into sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Markov Models**\n",
    "- **First-Order Markov Model**:\n",
    "   - Predicts the next word using only the **immediate preceding word**.\n",
    "   - Probability of a sentence is the product of the probabilities of generating each word conditioned on the previous word.\n",
    "\n",
    "### **Steps to Generate a Sentence**:\n",
    "1. Start with a **beginning symbol** `<beg>`.\n",
    "2. Sample the first word using a probability table.\n",
    "3. Use each generated word to predict the next word until the **end symbol** `<end>` is reached\n",
    "\n",
    "### **Maximum Likelihood Estimation**:\n",
    "- To train the Markov model:\n",
    "   - Count pairs of successive words across a corpus.\n",
    "   - Normalize these counts to derive probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Feed-Forward Neural Networks for Sequence Modeling**\n",
    "- **Transition from Markov Models**:\n",
    "   - Replace the fixed probability table with a **feed-forward neural network**.\n",
    "   - Input: One-hot vector of the previous word.\n",
    "   - Output: A probability distribution over the next word using **softmax**\n",
    "\n",
    "- **Extension to Higher Orders**:\n",
    "   - Include multiple preceding words as inputs.\n",
    "   - Introduce **hidden layers** to model complex combinations of preceding words.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Recurrent Neural Networks (RNNs)**\n",
    "- RNNs generalize Markov models and feed-forward networks by allowing:\n",
    "   1. **Variable-length history**: Retains information from earlier steps.\n",
    "   2. **State Persistence**: State $ s_t $ summarizes all previous words and updates with new input $ x_t $:  \n",
    "      $\n",
    "      s_t = \\tanh(W_{ss} s_{t-1} + W_{sx} x_t)\n",
    "      #\n",
    "- **Architecture**:\n",
    "   - Input: One-hot vector of the previous word.\n",
    "   - Hidden State: Maintains the history of the sequence.\n",
    "   - Output: Probability distribution over the next word via **softmax**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Training RNNs**\n",
    "- **Backpropagation Through Time (BPTT)**:\n",
    "   - Compute gradients of the loss function at each time step.\n",
    "   - Update parameters using stochastic gradient descent.\n",
    "\n",
    "- **Issues**:\n",
    "   - **Vanishing Gradients**: Gradients shrink over long sequences.\n",
    "   - **Exploding Gradients**: Gradients grow excessively, causing instability.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Advanced Sequence Models**\n",
    "- **Gated RNNs**:\n",
    "   - Add mechanisms (gates) to control how information is retained or overwritten in the hidden state.\n",
    "- **Long Short-Term Memory (LSTM)**:\n",
    "   - Introduces gates:\n",
    "      - **Forget Gate**: Controls what information to discard.\n",
    "      - **Input Gate**: Controls what new information to add.\n",
    "      - **Output Gate**: Controls what information to expose as output.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Sequence-to-Sequence Tasks**\n",
    "- **Encoding and Decoding**:\n",
    "   - **Encoder**: Turns sequences (e.g., sentences, images) into vectors.\n",
    "   - **Decoder**: Converts vectors back into sequences.\n",
    "   - Example: Translating a sentence from one language to another.\n",
    "\n",
    "### **Key Steps**:\n",
    "1. Start with a vector (initial state) instead of zeros.\n",
    "2. Use RNN (or LSTM) to sequentially generate words, conditioned on the state and previously generated words.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Applications**\n",
    "- **Language Modeling**: Predict the next word or character in a sentence.\n",
    "- **Sentiment Analysis**: Summarize a sentence into a vector to predict sentiment.\n",
    "- **Machine Translation**: Convert input vectors into sequences in another language.\n",
    "- **Image Captioning**: Encode an image into a vector and decode it into a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Key Takeaways**\n",
    "- RNNs overcome limitations of fixed-length history in Markov models.\n",
    "- They maintain a hidden state that evolves over time, enabling sequence predictions.\n",
    "- Advanced models like LSTMs handle long sequences better with gates for retaining or forgetting information.\n",
    "- RNNs can be extended to sequence-to-sequence tasks, such as translation and captioning, by encoding and decoding vector representations.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
