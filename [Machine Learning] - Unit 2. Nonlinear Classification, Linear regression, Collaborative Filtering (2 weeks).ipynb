{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 5. Linear Regression**\n",
    "\n",
    "# **Summary of Linear Regression and Regularization**\n",
    "\n",
    "## **1. Introduction to Linear Regression**\n",
    "- **Goal**: Learn a **linear mapping** from input features $ x $ to continuous values $ y $.\n",
    "- **Setup**:\n",
    "   - Input: Feature vectors $ x \\in \\mathbb{R}^d $.\n",
    "   - Output: Continuous label $ y \\in \\mathbb{R} $.\n",
    "   - **Linear Form**:  \n",
    "     $\n",
    "     f(x) = \\theta^\\top x + \\theta_0\n",
    "     $\n",
    "     where $ \\theta $ is the parameter vector and $ \\theta_0 $ is the offset.\n",
    "\n",
    "### **Why Linear Regression?**\n",
    "- Though simple, linear regression works well when:\n",
    "   - Features are carefully designed.\n",
    "   - Problems are appropriately transformed into a **feature space** where linear functions suffice.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Defining the Objective**\n",
    "- The objective of linear regression is to minimize the **Empirical Risk**:\n",
    "   - Measures the deviation between predicted and true values:\n",
    "     $\n",
    "     R_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left( y_i - \\theta^\\top x_i \\right)^2\n",
    "     $\n",
    "- **Squared Error Loss**:\n",
    "   - Penalizes larger deviations more heavily, ensuring sensitivity to large prediction errors.\n",
    "\n",
    "### **Two Types of Mistakes**:\n",
    "1. **Structural Errors**: Linear regression cannot model nonlinear relationships.\n",
    "2. **Estimation Errors**: Limited or noisy data leads to poorly estimated parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Solving Linear Regression**\n",
    "### **Gradient-Based Approach**\n",
    "- Iteratively update parameters in the direction of the negative gradient:\n",
    "   $\n",
    "   \\theta \\leftarrow \\theta - \\eta \\nabla R_n(\\theta)\n",
    "   $\n",
    "   where $ \\eta $ is the learning rate.\n",
    "\n",
    "### **Closed-Form Solution**\n",
    "- When the empirical risk is **convex**, the solution can be computed analytically:\n",
    "   $\n",
    "   \\theta = A^{-1} d\n",
    "   $\n",
    "   - $ A $: Covariance matrix of the features.\n",
    "   - $ d $: Feature-label correlation vector.\n",
    "\n",
    "- **Conditions**:\n",
    "   - The number of training examples $ n $ must be larger than the dimensionality $ d $ for $ A $ to be invertible.\n",
    "   - Computational cost: $ O(d^3) $.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Regularization in Linear Regression**\n",
    "- **Goal**: Improve generalization by preventing overfitting to noisy training data.\n",
    "\n",
    "### **Ridge Regression (L2 Regularization)**\n",
    "- Adds a **penalty** for large parameter values to the loss function:\n",
    "   $\n",
    "   R_n^\\lambda(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2} \\left( y_i - \\theta^\\top x_i \\right)^2 + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "   $\n",
    "   - **Effect of $ \\lambda $**:\n",
    "     - Large $ \\lambda $: Prioritizes small \\( \\theta \\), ignoring small variations in the data.\n",
    "     - Small $ \\lambda $: Focuses on fitting training data more closely.\n",
    "\n",
    "### **Regularized Gradient Update**:\n",
    "- Adjust gradient updates to include regularization:\n",
    "   $\n",
    "   \\theta \\leftarrow \\theta - \\eta \\left( \\nabla R_n(\\theta) + \\lambda \\theta \\right)\n",
    "   $\n",
    "   - Pushes parameters $ \\theta $ towards zero while optimizing the loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Benefits of Regularization**\n",
    "- Reduces **estimation errors** by discouraging overfitting to noise.\n",
    "- Balances fitting the training data and keeping parameters small.\n",
    "- Regularization introduces a \"sweet spot\" where test error is minimized, even if training error is slightly higher.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Takeaways**\n",
    "- Linear regression maps input features to continuous outputs using a linear function.\n",
    "- The objective minimizes the **squared error loss**, balancing structural and estimation errors.\n",
    "- **Gradient-based methods** and **closed-form solutions** are used for parameter optimization.\n",
    "- **Regularization** (e.g., ridge regression) improves generalization by penalizing large parameters, reducing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 6. Nonlinear Classification**\n",
    "\n",
    "### **Summary of Non-Linear Classification and Kernel Methods**\n",
    "\n",
    "## **1. Introduction to Non-Linear Classification**\n",
    "- **Goal**: Extend linear classifiers to solve **non-linear problems** using feature transformations.\n",
    "- **Key Idea**:\n",
    "   - Map input $ x $ into a higher-dimensional **feature space** $ \\phi(x) $.\n",
    "   - Perform **linear classification** in the new space, which corresponds to a **non-linear decision boundary** in the original input space.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Feature Transformation for Non-Linearity**\n",
    "- **Feature Maps**:\n",
    "   - Transform $ x $ into new features, e.g., $ \\phi(x) = [x, x^2, x^3, \\dots] $.\n",
    "   - Higher-order features (e.g., cross-terms) allow for more expressive decision boundaries.\n",
    "\n",
    "### **Example**:\n",
    "1. **1D Input**: $ x $ mapped to $ \\phi(x) = [x, x^2] $.\n",
    "   - Linear classification in the transformed space results in a **quadratic decision boundary**.\n",
    "2. **2D Input**: $ x = [x_1, x_2] $ with features $ \\phi(x) = [x_1, x_2, x_1^2, x_2^2, x_1x_2] $.\n",
    "   - The feature space becomes 5-dimensional.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. The Curse of Dimensionality**\n",
    "- Adding higher-order terms increases the dimensionality rapidly:\n",
    "   - $ d $-dimensional input with up to $ p $-order features results in $ O(d^p) $ features.\n",
    "- **Challenge**: Explicitly constructing high-dimensional feature vectors is computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Kernel Methods**\n",
    "- **Solution**: Avoid explicit feature construction by operating directly on **inner products** in the feature space using **kernel functions**.\n",
    "\n",
    "### **Kernel Function**:\n",
    "- A kernel $ K(x, x') $ computes the inner product of $ \\phi(x) $ and $ \\phi(x') $ implicitly:\n",
    "   $\n",
    "   K(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\n",
    "   $\n",
    "- Examples:\n",
    "   1. **Linear Kernel**: $ K(x, x') = x^\\top x' $\n",
    "   2. **Polynomial Kernel**: $ K(x, x') = (1 + x^\\top x')^p $\n",
    "   3. **Radial Basis Function (RBF) Kernel**:  \n",
    "      $\n",
    "      K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right)\n",
    "      $\n",
    "      - Provides infinite-dimensional feature representations.\n",
    "\n",
    "### **Benefits**:\n",
    "- Kernels allow non-linear transformations **without explicitly computing features**.\n",
    "- Computational cost depends only on the kernel function.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Kernel Perceptron Algorithm**\n",
    "- Perceptron updates expressed using kernel functions:\n",
    "   $\n",
    "   \\theta = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x)\n",
    "   $\n",
    "   where $ \\alpha_j $ counts the mistakes made on training samples.\n",
    "\n",
    "### **Steps**:\n",
    "1. Initialize $ \\alpha_j = 0 $ for all $ j $.\n",
    "2. For each training sample $ x_i $, check if a mistake is made.\n",
    "3. If a mistake occurs, update $ \\alpha_i $ by $ 1 $.\n",
    "\n",
    "### **Interpretation**:\n",
    "- Kernel $ K(x, x_j) $ measures the similarity between $ x $ and the $ j $-th training sample.\n",
    "- Predictions are made using a weighted sum of kernel evaluations between the test point and training samples.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Radial Basis Function (RBF) Kernel**\n",
    "- The RBF kernel creates a **smooth, nonlinear decision boundary** in the input space.\n",
    "- **Key Property**:\n",
    "   - Even with infinite features, the RBF kernel is computationally efficient.\n",
    "- **Application**:\n",
    "   - RBF kernels can perfectly separate any linearly separable data with sufficient training iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Decision Trees and Random Forests**\n",
    "- **Decision Trees**:\n",
    "   - Create **axis-aligned splits** in the input space to form non-linear decision boundaries.\n",
    "- **Random Forests**:\n",
    "   - Combine multiple decision trees with randomness:\n",
    "     - Randomly select features for splits.\n",
    "     - Use **bootstrap samples** of the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Key Takeaways**\n",
    "- **Feature transformations** enable non-linear classification by mapping inputs to a higher-dimensional space.\n",
    "- **Kernel methods** allow implicit computation in high-dimensional spaces using kernel functions, avoiding explicit feature construction.\n",
    "- The **Kernel Perceptron** algorithm uses kernel functions to efficiently solve non-linear classification problems.\n",
    "- **RBF kernels** are especially powerful, offering infinite-dimensional representations with computational efficiency.\n",
    "- **Alternative Non-Linear Methods**:\n",
    "   - Decision trees and random forests provide interpretable, axis-aligned non-linear classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 7. Recommender Systems**\n",
    "\n",
    "### **Summary of Recommender Systems**\n",
    "\n",
    "## **1. Introduction to Recommender Systems**\n",
    "- **Goal**: Predict user preferences for items (e.g., movies, products) based on prior behavior.\n",
    "- Applications: Widely used in e-commerce (e.g., Amazon, Netflix).\n",
    "- The problem involves a sparse matrix $ Y $ with user preferences for items, where most entries are missing.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Problem Definition**\n",
    "- $ Y $: $ n \\times m $ matrix (users $ n $, items $ m $).\n",
    "- Objective: Predict missing entries in $ Y $, filling it into a complete matrix $ X $.\n",
    "- Challenges:\n",
    "  - Sparsity: Few ratings compared to the size of the matrix.\n",
    "  - Feature extraction: Not always clear which features determine preferences.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. K-Nearest Neighbors (KNN) for Recommendations**\n",
    "- **Basic Idea**:\n",
    "  - Identify $ K $ -nearest users to a target user based on similarity (e.g., cosine similarity).\n",
    "  - Predict ratings as the weighted average of neighbors' ratings.\n",
    "- **Limitations**:\n",
    "  - Fails to capture hidden patterns (e.g., a user liking both gardening and machine learning books).\n",
    "  - Struggles with complex relationships between users and items.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Collaborative Filtering with Matrix Factorization**\n",
    "- **Key Assumption**: The user-item matrix $ X $ is low-rank, reflecting latent factors (e.g., user preferences and item attributes).\n",
    "\n",
    "### **Low-Rank Decomposition**:\n",
    "1. Factorize $ X $ into two matrices:\n",
    "   - $ U $: User features ($ n \\times k $).\n",
    "   - $ V $: Item features ($ m \\times k $).\n",
    "   - $ X = U \\cdot V^\\top $, where $ k $ is the rank.\n",
    "2. Significantly reduces the number of parameters from $ n \\times m $ to $ n \\times k + m \\times k $.\n",
    "\n",
    "### **Interpretation**:\n",
    "- $ U $: Represents user preferences across $ k $ latent factors.\n",
    "- $ V $: Represents item characteristics across $ k $ latent factors.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Optimization Problem**\n",
    "- **Objective**:\n",
    "  - Minimize the reconstruction error between known entries of $ Y $ and the predicted matrix $ X $:\n",
    "    $\n",
    "    J(U, V) = \\sum_{(a, i) \\in D} (Y_{ai} - U_a \\cdot V_i^\\top)^2 + \\lambda (\\|U\\|^2 + \\|V\\|^2)\n",
    "    $\n",
    "  - Regularization term ($ \\lambda $): Prevents overfitting by penalizing large values in $ U $ and $ V $.\n",
    "\n",
    "- **Algorithm**:\n",
    "  - Alternating minimization:\n",
    "    1. Fix $ V $, optimize $ U $.\n",
    "    2. Fix $ U $, optimize $ V $.\n",
    "  - Repeat until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Example of Matrix Factorization**\n",
    "- **Rank-1 Example**:\n",
    "  - Initialize $ V $ randomly.\n",
    "  - Update $ U $ to minimize loss for observed entries in $ Y $.\n",
    "  - Iteratively update $ U $ and $ V $ until convergence.\n",
    "- Results:\n",
    "  - $ U $ and $ V $ encode latent factors, allowing $ X $ to be reconstructed.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Advantages of Matrix Factorization**\n",
    "1. **Captures Latent Patterns**:\n",
    "   - Discovers hidden relationships among users and items.\n",
    "   - Handles multi-dimensional preferences (e.g., user preferences across different genres).\n",
    "2. **Scalable**:\n",
    "   - Operates efficiently even for large matrices by reducing the number of parameters.\n",
    "3. **Generalizable**:\n",
    "   - Extends to higher ranks $ k $ for richer representations.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Key Takeaways**\n",
    "- Recommender systems predict user-item interactions by leveraging past data.\n",
    "- Simple methods like KNN provide basic recommendations but lack the ability to uncover hidden patterns.\n",
    "- Matrix factorization (e.g., collaborative filtering) effectively models complex user-item relationships by factorizing the preference matrix into latent factors.\n",
    "- Regularization and iterative optimization ensure robust performance, avoiding overfitting and enabling scalability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
