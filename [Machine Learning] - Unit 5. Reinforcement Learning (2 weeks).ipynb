{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 5. Reinforcement Learning (2 weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lecture 17. Reinforcement Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Markov decision process (MDP)** is defined by:\n",
    "\n",
    "- States: a set of states $ s \\in S $; \n",
    "- Actions: a set of actions $ a \\in A $;\n",
    "- Action dependent transition probabilities:\n",
    "\n",
    "$$\n",
    "T(s, a, s') = P(s' \\mid s, a), \\quad \\text{so that for each state } s \\text{ and action } a,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{s' \\in S} T(s, a, s') = 1.\n",
    "$$\n",
    "\n",
    "- Reward functions \\( R(s, a, s') \\), representing the reward for starting in state \\( s \\), taking action \\( a \\), and ending up in state \\( s' \\) after one step. (The reward function may also depend only on \\( s \\), or only \\( s \\) and \\( a \\).)\n",
    "\n",
    "MDPs satisfy the **Markov property** in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history (i.e. past states and actions) that leads to the current state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Finite horizon based utility**\n",
    "The utility function is the sum of rewards after acting for a fixed number \\( n \\) steps. For example, when the rewards depend only on the states, the utility function is defined as:\n",
    "\n",
    "$\n",
    "U[s_0, s_1, \\dots, s_n] = \\sum_{i=0}^n R(s_i) \\quad \\text{for some fixed number of steps \\( n \\).}\n",
    "$\n",
    "\n",
    "In particular, for any positive integer $ m $:\n",
    "\n",
    "$\n",
    "U[s_0, s_1, \\dots, s_{n+m}] = U[s_0, s_1, \\dots, s_n].\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **(Infinite horizon) discounted reward based utility**\n",
    "In this setting, the reward one step into the future is **discounted** by a factor $ \\gamma $, the reward two steps ahead by $ \\gamma^2 $, and so on. The goal is to continue acting (without an end) while maximizing the expected discounted reward. The discounting allows us to focus on near-term rewards and control this focus by changing $ \\gamma $.\n",
    "\n",
    "For example, if the rewards depend only on the states, the utility function is defined as:\n",
    "\n",
    "$\n",
    "U[s_0, s_1, \\dots] = \\sum_{k=0}^\\infty \\gamma^k R(s_k).\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Terms\n",
    "- $ \\gamma $: Discount factor where $ 0 \\leq \\gamma \\leq 1 $.\n",
    "   - $ \\gamma $ close to 0: Emphasizes immediate rewards (short-term focus).\n",
    "   - $ \\gamma $ close to 1: Considers future rewards more heavily (long-term focus).\n",
    "- $ R(s_i) $: Reward at state $ s_i $.\n",
    "- $ k $: Time step.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| **Utility Type**                 | **Definition**                                  | **Time Horizon** | **Key Characteristic**               |\n",
    "|----------------------------------|-----------------------------------------------|------------------|--------------------------------------|\n",
    "| **Finite Horizon Utility**       | Sum of rewards over a fixed \\( n \\) steps.     | Finite           | Ends after \\( n \\) steps.            |\n",
    "| **Discounted Reward Utility**    | Sum of discounted rewards over infinite steps. | Infinite         | Applies a discount factor \\( \\gamma \\). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture the **Bellman Equations** are:\n",
    "\n",
    "$\n",
    "V^*(s) = \\max_a Q^*(s, a)\n",
    "$\n",
    "\n",
    "$\n",
    "Q^*(s, a) = \\sum_{s'} T(s, a, s') \\left( R(s, a, s') + \\gamma V^*(s') \\right)\n",
    "$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{- the **value function** } V^*(s) \\text{ is the expected reward from starting at state } s \\text{ and acting optimally.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{- the **Q-function** } Q^*(s, a) \\text{ is the expected reward from starting at state } s, \\text{ then acting with action } a, \\text{ and acting optimally afterwards.}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture the **value iteration update rule**:\n",
    "\n",
    "$\n",
    "V_{k+1}^*(s) = \\max_a \\left[ \\sum_{s'} T(s, a, s') \\left( R(s, a, s') + \\gamma V_k^*(s') \\right) \\right],\n",
    "$\n",
    "\n",
    "where $ V_k^*(s) $ is the expected reward from state $ s $ after acting optimally for $ k $ steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "Recall the example discussed in the lecture:\n",
    "\n",
    "| Agent's starting state |   |   |   |   | **+1** |\n",
    "|------------------------|---|---|---|---|--------|\n",
    "\n",
    "An agent is trying to navigate a one-dimensional grid consisting of **5 cells**. At each step, the agent has only **one action** to choose from (i.e., it moves to the cell on the immediate right).\n",
    "\n",
    "---\n",
    "\n",
    "### Reward Function:\n",
    "- The reward function is defined as \\( R(s, a, s') = R(s) \\).\n",
    "- \\( R(s = 5) = 1 \\) and \\( R(s) = 0 \\) otherwise.\n",
    "\n",
    "**Note**: The agent receives the reward **when leaving the current state**. When it reaches the rightmost cell (cell 5), it stays for one more step, receives a reward of **+1**, and comes to a halt.\n",
    "\n",
    "---\n",
    "\n",
    "### Value Function:\n",
    "- Let \\( V^*(i) \\) denote the value function of state \\( i \\) (the \\( i^{th} \\) cell starting from the left).\n",
    "- \\( V_k^*(i) \\) is the value function estimate at state \\( i \\) at the \\( k^{th} \\) step of the value iteration algorithm.\n",
    "- \\( V_0^*(i) \\) is the initialization of this estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### Discount Factor:\n",
    "Use the discount factor \\( \\gamma = 0.5 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### Notation:\n",
    "We write the functions \\( V_k^* \\) as arrays below, i.e., as:\n",
    "\n",
    "\\[\n",
    "\\left[ V_k^*(1) \\quad V_k^*(2) \\quad V_k^*(3) \\quad V_k^*(4) \\quad V_k^*(5) \\right].\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Initialization:\n",
    "Initialize by setting \\( V_0^*(i) = 0 \\) for all \\( i \\):\n",
    "\n",
    "\\[\n",
    "V_0^* = [0 \\quad 0 \\quad 0 \\quad 0 \\quad 0].\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Value Iteration:\n",
    "Using the value iteration update rule, we get:\n",
    "\n",
    "\\[\n",
    "V_1^* = [0 \\quad 0 \\quad 0 \\quad 0 \\quad 1],\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "V_2^* = [0 \\quad 0 \\quad 0 \\quad 0.5 \\quad 1].\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Final Note:\n",
    "**Note**: As soon as the agent takes the first action to reach **cell 5**, it stays for one more step, halts, and does not take any more action. Therefore:\n",
    "\n",
    "\\[\n",
    "V_{k+1}^*(5) = V_k^*(5) \\quad \\text{for all } k \\geq 1.\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 18. Reinforcement Learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Inputs for RL algorithm\n",
    "- The derivation of the Q-value iteration update rule from the equation above is similar to the derivation of the value iteration update rule.\n",
    "- Bellman Equations: First, recall the Bellman equations:\n",
    "- Derivation of Q-value Update Rule: Plugging the first equation into the second, we get:\n",
    "- Explanation: \n",
    "  1.1 Represents the expected reward from state  when taking action , and then following the optimal policy.\n",
    "  1.2 Transition probability of moving from state  to  after taking action .\n",
    "  1.3 Immediate reward received when transitioning from  to .\n",
    "  1.4 Discount factor (), controlling how future rewards are weighted.\n",
    "  1.5 The term  represents the optimal value of the next state  when taking the best possible action .\n",
    "\n",
    "- Iterative Computation\n",
    "To compute  iteratively:\n",
    "1.1 Initialize  for all states  and actions .\n",
    "1.2 Update using the Q-value iteration update rule:\n",
    "1.3 Repeat until the values converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning: Key Concepts and Topics\n",
    "\n",
    "1. Multi-Armed Bandit (MAB) Problem\n",
    "- Goal: Maximize cumulative rewards while balancing exploration vs. exploitation.\n",
    "- Strategies:\n",
    "* Random Selection: Uniform choice of actions without learning.\n",
    "* Explore-Then-Commit (ETC): Exploration phase followed by exploitation.\n",
    "* œµ-Greedy: Random exploration with a fixed probability.\n",
    "* Upper Confidence Bound (UCB): Optimistic exploration based on upper confidence bounds.\n",
    "- Regret Analysis: Quantifies deviation from optimal rewards over time.\n",
    "\n",
    "2. Sequential Decision Making\n",
    "- Markov Decision Process (MDP): Framework for RL with states, actions, transitions, and rewards.\n",
    "- Goal: Maximize cumulative rewards using a policy ùúã\n",
    "- Policy Gradient: Optimizing policies using gradient-based methods, suitable for continuous spaces.\n",
    "\n",
    "3. Policy Gradient Methods\n",
    "- Reinforce: Vanilla policy gradient that adjusts actions based on expected rewards.\n",
    "- Discount: Reduces variance by discounting future rewards.\n",
    "- Baselines: Introduces advantage values to reduce variance.\n",
    "- Actor-Critic: Combines value function estimation (critic) with policy optimization (actor).\n",
    "\n",
    "4. Reward Shaping\n",
    "- Sparse vs. Dense Rewards: Sparse rewards provide limited feedback; dense rewards guide agents efficiently.\n",
    "- Challenges: Agents may get stuck at local minima or exploit reward functions (reward hacking).\n",
    "- Solutions: Penalties for obstacles, sub-goals for navigation tasks.\n",
    "\n",
    "5. Learning from Demonstration (LfD)\n",
    "- Enables agents to learn by mimicking expert demonstrations, reducing learning time and resource costs.\n",
    "- Behavior Cloning (BC): Treats learning as supervised learning but struggles with covariate shift.\n",
    "- DAgger: Iterative training to address covariate shift by querying experts for new states.\n",
    "\n",
    "6. Bridging Simulation and Reality\n",
    "- Domain Randomization: Generalizes policies by randomizing simulation parameters.\n",
    "- System Identification: Fine-tunes policies by inferring real-world system parameters for adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Themes\n",
    "1. Balancing Exploration and Exploitation: Core to decision-making in RL (MAB and policy optimization).\n",
    "2. Handling Reward Design: Reward shaping is crucial but challenging for task success.\n",
    "3. Bridging Simulation-Real World Gap: Techniques like domain randomization and system identification enhance real-world applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 19: Applications: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is Natural Language Processing (NLP)?**\n",
    "- **Definition**: NLP enables machines to process and understand human language, from basic string matching to deep contextual comprehension.  \n",
    "- **Origins**: The **Turing Test** (1950s) introduced the idea of evaluating a machine's intelligence through human-like conversation&#8203;:contentReference[oaicite:0]{index=0}.  \n",
    "- **Early Systems**: MIT's *ELIZA* (1966) demonstrated simplistic dialogue, showing how humans could interpret repetitive machine outputs as meaningful&#8203;:contentReference[oaicite:1]{index=1}.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Evolution of NLP Approaches**  \n",
    "### **Symbolic vs. Statistical Methods**  \n",
    "- **Symbolic**: Focuses on hand-crafted grammar and rules to understand language.  \n",
    "- **Statistical**: Relies on data-driven learning to detect patterns without requiring deep understanding&#8203;:contentReference[oaicite:2]{index=2}.  \n",
    "\n",
    "### **Key Milestones**  \n",
    "- **Hidden Markov Model**: Fred Jelinek introduced **Hidden Markov Models** for speech processing, marking a major statistical breakthrough&#8203;:contentReference[oaicite:3]{index=3}.  \n",
    "- **Penn Treebank**: Released in 1993, the **Penn Treebank** enabled machine learning models to train on syntactically parsed text&#8203;:contentReference[oaicite:4]{index=4}.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Applications of NLP**  \n",
    "NLP has revolutionized many areas, including:  \n",
    "- **Practical Applications**: Search engines, machine translation, sentiment analysis, information extraction, and text generation&#8203;:contentReference[oaicite:5]{index=5}&#8203;:contentReference[oaicite:6]{index=6}.  \n",
    "- **Societal Impact**: NLP powers essay grading for GRE/SAT, structures databases, and automates news summaries&#8203;:contentReference[oaicite:7]{index=7}.  \n",
    "- **Current Capabilities**: Tasks such as spam detection, named entity recognition (NER), and fact-based question answering are well-solved&#8203;:contentReference[oaicite:8]{index=8}.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Challenges in NLP**  \n",
    "### **Ambiguity**  \n",
    "- Examples: *\"Iraqi head seeks arms\"* or *\"A computer that understands you like your mother\"*.  \n",
    "  - Resolving syntactic and semantic ambiguity remains a challenge&#8203;:contentReference[oaicite:9]{index=9}.  \n",
    "\n",
    "### **Data Dependency**  \n",
    "- Performance heavily depends on the availability of **task-specific training data**.  \n",
    "- Example: Machine translation works well for news text but fails for recipes&#8203;:contentReference[oaicite:10]{index=10}.  \n",
    "\n",
    "### **Complex Tasks**  \n",
    "- **Unconstrained Question Answering**: Requires logical reasoning and data analysis, making it highly challenging.  \n",
    "- **Advanced Dialogue Systems**: Effective in narrow domains but struggle with general conversation.  \n",
    "- **Text Summarization**: Summarizing books or articles with key points remains difficult&#8203;:contentReference[oaicite:11]{index=11}&#8203;:contentReference[oaicite:12]{index=12}.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Future Directions**  \n",
    "- Improving **machine reasoning** and contextual comprehension is a major focus of ongoing research.  \n",
    "- **Summarization and Evidence-based NLP**: NLP holds potential for tasks like summarizing medical studies or resolving contradictory information.  \n",
    "   - Example: Resolving debates such as *\"Does coffee cause cancer?\"* by analyzing various research findings&#8203;:contentReference[oaicite:13]{index=13}.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**  \n",
    "- **NLP Progress**: Modern NLP systems excel at specific tasks like translation and sentiment analysis but struggle with general reasoning and complex discourse.  \n",
    "- **Role of Machine Learning**: Machine learning has revolutionized NLP by enabling systems to learn from vast data instead of relying on hand-coded rules.  \n",
    "- **Remaining Challenges**: Ambiguity, data limitations, and logical reasoning tasks remain key hurdles for NLP advancements.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
