{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 4. Unsupervised Learning (2 weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 13. Clustering 1\n",
    "\n",
    "### **1. Introduction to Clustering**\n",
    "- **Clustering**: The process of partitioning a set of feature vectors into groups (clusters) based on similarity&#8203;:contentReference[oaicite:0]{index=0}.\n",
    "- **Two Views of Clustering**:  \n",
    "   1. **Partitioning**: Grouping data into disjoint subsets.  \n",
    "   2. **Representatives**: Selecting a representative point (e.g., centroid) to represent each cluster.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Defining Clustering Cost**\n",
    "- **Clustering Cost**: A measure to evaluate how well the data is partitioned into clusters&#8203;:contentReference[oaicite:1]{index=1}.  \n",
    "- **Methods to Define Cost**:\n",
    "   - **Diameter**: Distance between the most remote points.  \n",
    "   - **Average Distance**: Average distance among all points in a cluster.  \n",
    "   - **Representative Distance**: Sum of distances between points and a representative (e.g., centroid).  \n",
    "\n",
    "### **Common Distance Measures**  \n",
    "1. **Euclidean Distance (L2 Norm)**:  \n",
    "   - Measures straight-line distance between two points.  \n",
    "   - Squared Euclidean Distance simplifies optimization in K-means.  \n",
    "   $\n",
    "   d(x_i, x_j) = \\sqrt{\\sum_{k=1}^n (x_{i,k} - x_{j,k})^2}\n",
    "   $\n",
    "\n",
    "2. **Manhattan Distance (L1 Norm)**:  \n",
    "   - Measures the sum of absolute differences along each dimension.  \n",
    "   $\n",
    "   d(x_i, x_j) = \\sum_{k=1}^n |x_{i,k} - x_{j,k}|\n",
    "   $\n",
    "\n",
    "3. **Cosine Similarity**:  \n",
    "   - Measures the cosine of the angle between two vectors.  \n",
    "   - Sensitive to direction but not magnitude.  \n",
    "   $\n",
    "   \\text{Similarity}(x_i, x_j) = \\frac{x_i \\cdot x_j}{\\|x_i\\| \\|x_j\\|}\n",
    "   $\n",
    "   - A **distance version** can be expressed as:  \n",
    "   $\n",
    "   d(x_i, x_j) = 1 - \\text{Similarity}(x_i, x_j)\n",
    "   $\n",
    "---\n",
    "\n",
    "### **3. The K-Means Algorithm**\n",
    "### **Key Steps**:\n",
    "1. **Initialization**: Randomly select K cluster centers (representatives).  \n",
    "2. **Iterative Updates**:  \n",
    "   - **Step 1**: Assign each point to the nearest cluster center based on distance.  \n",
    "   - **Step 2**: Recompute the cluster center (centroid) for each cluster.  \n",
    "3. **Convergence**: Repeat Steps 1 and 2 until the clustering stabilizes.  \n",
    "\n",
    "### **Centroid Calculation**:  \n",
    "- The cluster representative is the mean (centroid) of all points within the cluster:  \n",
    "   $\n",
    "   z_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i\n",
    "   $  \n",
    "- This derivation works with **Euclidean Squared Distance** but may differ with other distance measures.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Properties of K-Means**\n",
    "### **Convergence**:\n",
    "- The algorithm **converges** to a local minimum because the cost function decreases monotonically.  \n",
    "- The number of iterations is finite since the partitions are finite.\n",
    "\n",
    "### **Sensitivity to Initialization**:\n",
    "- Poor initialization can result in suboptimal clustering with a high cost.  \n",
    "- Example: If centroids are initialized close together, some clusters may overlap while others remain underrepresented.  \n",
    "\n",
    "### **Mitigating Initialization Issues**:\n",
    "- **Multiple Runs**: Run the algorithm multiple times with different random initializations.  \n",
    "- **Improved Initialization**: Spread centroids to reduce the chance of overlap.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Impact of Distance Measures in K-Means**\n",
    "- **Euclidean Distance**: Standard in K-means; computationally efficient.  \n",
    "- **Manhattan Distance**: Robust to outliers, suitable for grid-like data.  \n",
    "- **Cosine Similarity**: Effective for high-dimensional, sparse data (e.g., text data).  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary of Key Points**\n",
    "- K-means clusters data by iteratively minimizing the distance between points and their cluster centers.\n",
    "- The choice of **distance measure** significantly impacts clustering results and performance.\n",
    "- **Euclidean Squared Distance** is standard for K-means, but other measures (Manhattan, Cosine, Minkowski, etc.) may be better suited for specific applications.\n",
    "- Convergence is guaranteed, but sensitivity to initialization can lead to suboptimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 14. Clustering 2\n",
    "### **Summary of K-Medoids Algorithm and Determining K**\n",
    "\n",
    "## **1. Review of K-Means Limitations**\n",
    "- K-means relies on:\n",
    "  - **Centroids**: Representatives calculated as the mean of a cluster.  \n",
    "  - **Squared Euclidean Distance**: A necessary assumption for centroid computation\n",
    "- **Limitations**:\n",
    "  1. **Representatives**: Centroids may not be actual data points, causing issues in applications like visualization.\n",
    "  2. **Distance Metric**: K-means only works with squared Euclidean distance\n",
    "---\n",
    "\n",
    "## **2. Introduction to K-Medoids**\n",
    "- K-medoids is a variant of K-means that addresses its limitations.  \n",
    "- **Key Features**:\n",
    "  - **Representatives**: Must be actual data points from the input set.\n",
    "  - **Distance Metrics**: Supports any distance function, not limited to squared Euclidean distance.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. K-Medoids Algorithm**\n",
    "### **Steps**:\n",
    "1. **Initialization**: Randomly select K representatives (medoids) from the input points.  \n",
    "2. **Iteration**:\n",
    "   - **Step 2a**: Assign each point to the closest medoid based on a chosen distance metric.  \n",
    "   - **Step 2b**: Update the medoid for each cluster to minimize the total distance to all cluster members&#8203;:contentReference[oaicite:3]{index=3}.  \n",
    "     - The medoid is chosen such that:  \n",
    "       $\n",
    "       z_j = \\text{argmin} \\sum_{x_i \\in C_j} d(z_j, x_i)\n",
    "       $\n",
    "3. **Repeat** until the cost converges (no change in cluster assignments or medoids).  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Complexity Comparison: K-Means vs. K-Medoids**\n",
    "| **Algorithm** | **Cost per Iteration**         | **Explanation**                                |\n",
    "|---------------|--------------------------------|-----------------------------------------------|\n",
    "| K-Means       | $ O(nkd) $                   | Distance computation for $ n $ points, $ k $ clusters, $ d $-dimensions. |\n",
    "| K-Medoids     | $ O(n^2 kd) $                | Requires pairwise distance computation, making it more expensive. |\n",
    "\n",
    "- **Trade-offs**:\n",
    "  - K-means is faster but less flexible.\n",
    "  - K-medoids is computationally expensive but works with any distance measure and ensures representative points are from the data set.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Determining the Number of Clusters (K)**\n",
    "### **Challenges**:\n",
    "- K is often unknown and must be selected based on the application or data context.\n",
    "\n",
    "### **Cost-K Relationship**:\n",
    "- Increasing K reduces clustering cost.  \n",
    "- When $ K = n $ (each point is its own cluster), the cost becomes zero.  \n",
    "- **Trade-off**: Too large $ K $ may overfit the data, while too small $ K $ may oversimplify.\n",
    "\n",
    "### **Strategies for Determining K**:\n",
    "1. **Domain Knowledge**:\n",
    "   - If $ K $ is given or constrained (e.g., dividing a class into recitations), it is straightforward.  \n",
    "2. **Cost Analysis**:\n",
    "   - Plot cost vs. $ K $ to identify the \"elbow point,\" where increasing $ K $ yields diminishing cost improvements.  \n",
    "3. **Performance-Based Selection**:\n",
    "   - Use clustering as a preprocessing step for supervised tasks.\n",
    "   - Select $ K $ based on improved performance on a development dataset.  \n",
    "   - Example: Evaluate cluster distance features to optimize supervised learning performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. The Role of Indirect Supervision in Clustering**\n",
    "- **Clustering Algorithms**: Considered \"unsupervised\" but require human decisions:  \n",
    "  - Choosing **similarity measures**.  \n",
    "  - Determining the **number of clusters** $ K $.  \n",
    "  - Designing data representations (e.g., weighting words for text clustering).  \n",
    "- These decisions significantly influence clustering outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- K-medoids improves upon K-means by ensuring representatives are actual data points and supporting any distance metric.  \n",
    "- K-medoids is computationally more expensive but flexible.  \n",
    "- Determining $ K $ is often application-specific and requires balancing cost reduction and practical utility.  \n",
    "- Clustering involves implicit supervision through design choices, such as similarity metrics and representation methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 15. Generative Models**  \n",
    "### Summary of Generative Models and Gaussian Estimation\n",
    "\n",
    "## **1. Generative Models Overview**\n",
    "- **Definition**:  \n",
    "  - Generative models describe the underlying structure of positive and negative classes probabilistically.  \n",
    "  - Unlike discriminative models, which focus on finding boundaries, generative models estimate the distribution for each class\n",
    "\n",
    "- **Two Model Types Discussed**:  \n",
    "  1. **Multinomials**: Commonly used to model text data, where words are generated independently.  \n",
    "  2. **Gaussians**: Describe data as clusters with mean (center) and variance (spread)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Multinomial Models**\n",
    "### **Key Concepts**:\n",
    "- **Vocabulary**: Fixed set of words (or symbols) from which the model generates words.\n",
    "- **Parameters**:  \n",
    "  - $ \\theta_w $: Probability of generating word $ w $.  \n",
    "  - Constraints: $ \\theta_w \\geq 0 $ and $ \\sum_{w \\in W} \\theta_w = 1 $.\n",
    "\n",
    "### **Likelihood of a Document**:\n",
    "- For a document $ D $ with word counts $ \\text{count}(w) $, the likelihood is:  \n",
    "   $\n",
    "   P(D|\\theta) = \\prod_{w \\in W} \\theta_w^{\\text{count}(w)}\n",
    "   $\n",
    "- **Simplified Form** (when words repeat):  \n",
    "   $\n",
    "   P(D|\\theta) = \\prod_{w \\in \\text{Vocabulary}} \\theta_w^{\\text{count}(w)}\n",
    "   $\n",
    "\n",
    "### **Parameter Estimation (Maximum Likelihood)**:\n",
    "- Estimate $ \\theta_w $ as:  \n",
    "   $\n",
    "   \\theta_w = \\frac{\\text{count}(w)}{\\sum_{w' \\in W} \\text{count}(w')}\n",
    "   $\n",
    "- For two-word vocabulary (0 and 1):  \n",
    "   $\n",
    "   \\theta_0 = \\frac{\\text{count}(0)}{\\text{count}(0) + \\text{count}(1)}, \\quad \\theta_1 = 1 - \\theta_0\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Gaussian Models**\n",
    "### **Introduction**:\n",
    "- Gaussians describe a **cloud of points** in $ R^d $ using two parameters:  \n",
    "   - **Mean ($ \\mu $)**: Center of the cloud.  \n",
    "   - **Variance ($ \\sigma^2 $)**: Spread or dispersion of the points.\n",
    "\n",
    "### **Gaussian Probability Distribution**:\n",
    "- The likelihood of a point $ x $ being generated by a Gaussian is:  \n",
    "   $\n",
    "   P(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "   $  \n",
    "- Points farther from the mean have lower probabilities.\n",
    "\n",
    "### **Parameter Estimation**:\n",
    "- **Goal**: Find $ \\mu $ and $ \\sigma^2 $ that maximize the likelihood of the data.  \n",
    "- **Log-Likelihood Simplification**: Taking the log of the likelihood simplifies the computation.  \n",
    "- **Optimal Parameters**:\n",
    "   - Mean $( \\mu \\$:  \n",
    "     $\n",
    "     \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "     $\n",
    "   - Variance $( \\sigma^2 \\$:  \n",
    "     $\n",
    "     \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2\n",
    "     $.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Incorporating Priors in Classification**\n",
    "- Using **Bayes' Rule**, we can incorporate prior probabilities of classes (e.g., positive or negative).  \n",
    "- The likelihood of a document $ d $ belonging to a class $ y $:  \n",
    "   $\n",
    "   P(y | d) \\propto P(d | y) P(y)\n",
    "   $\n",
    "- Taking the **log** simplifies the expression into:  \n",
    "   $\n",
    "   \\log \\frac{P(y=+ | d)}{P(y=- | d)} = \\text{sum of word weights} + \\text{log prior ratio}\n",
    "   $\n",
    "- Results in a **linear classifier** with an offset guided by priors&#8203;:contentReference[oaicite:8]{index=8}.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Prediction in Generative Models**\n",
    "- **Objective**: Given a new document or point, assign it to the class that gives it the highest likelihood.  \n",
    "- **Approach**:\n",
    "   - Compute the likelihood for each class.  \n",
    "   - Compare likelihoods and choose the class with the maximum value.  \n",
    "   - Example:  \n",
    "     - Classify a document $ D $ based on multinomial parameters $ \\theta $ for each class.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Takeaways**\n",
    "- Generative models focus on estimating class distributions to perform classification.  \n",
    "- **Multinomials** are well-suited for text, while **Gaussians** describe continuous data in $ R^d $.  \n",
    "- Parameter estimation uses **maximum likelihood**, often simplified via log transformations.  \n",
    "- Incorporating priors enables models to adjust for class imbalances.  \n",
    "- Despite their probabilistic nature, generative models can still result in linear classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 16. Mixture Models; EM algorithm**\n",
    "\n",
    "# **Summary of Mixture Models and the EM Algorithm**\n",
    "\n",
    "## **1. Overview of Mixture Models**\n",
    "- Mixture models describe data using **multiple clusters**, where each cluster is modeled as a Gaussian with its own parameters:\n",
    "  - **Mean ($ \\mu_j $)**: Center of the cluster.\n",
    "  - **Variance ($ \\sigma_j^2 $)**: Spread of the cluster.\n",
    "  - **Mixture Weights ($ p_j $)**: Probability of choosing a cluster.\n",
    "- The process of generating a point involves:\n",
    "  1. Selecting a cluster probabilistically (using mixture weights).\n",
    "  2. Drawing a point from the Gaussian of the chosen cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Parameters in Mixture Models**\n",
    "The parameters for $ K $ components include:\n",
    "- **Mixture Weights ($ p_j $)**: Sum to 1 across all clusters.\n",
    "- **Mean ($ \\mu_j $)**: Estimated cluster centers.\n",
    "- **Variance ($ \\sigma_j^2 $)**: Spread of each cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. The Expectation-Maximization (EM) Algorithm**\n",
    "### **Goal**:\n",
    "Estimate the parameters $ p_j, \\mu_j, \\sigma_j^2 $ of the Gaussian Mixture Model (GMM).\n",
    "\n",
    "### **Steps**:\n",
    "1. **Initialization**:  \n",
    "   - Randomly initialize $ \\mu_j, \\sigma_j^2, p_j $ for all $ j $.  \n",
    "   - A common method is to use **K-means** to initialize means.\n",
    "\n",
    "2. **E-Step (Expectation)**:  \n",
    "   - Compute the **posterior probability** that a point $ x_i $ belongs to cluster $ j $:  \n",
    "     $\n",
    "     p_{ji} = \\frac{p_j \\cdot \\mathcal{N}(x_i | \\mu_j, \\sigma_j^2)}{\\sum_{k=1}^K p_k \\cdot \\mathcal{N}(x_i | \\mu_k, \\sigma_k^2)}\n",
    "     $\n",
    "     - $ p_{ji} $: Probability of $ x_i $ belonging to cluster $ j $.  \n",
    "     - $ \\mathcal{N} $: Gaussian probability density function.\n",
    "\n",
    "3. **M-Step (Maximization)**:  \n",
    "   - Update parameters to maximize the likelihood based on current soft assignments:\n",
    "     - **Cluster size** $ n_j $:  \n",
    "       $\n",
    "       n_j = \\sum_{i=1}^n p_{ji}\n",
    "       $\n",
    "     - **Updated Means** ($ \\mu_j $):  \n",
    "       $\n",
    "       \\mu_j = \\frac{\\sum_{i=1}^n p_{ji} \\cdot x_i}{n_j}\n",
    "       $\n",
    "     - **Updated Variance** ($ \\sigma_j^2 $):  \n",
    "       $\n",
    "       \\sigma_j^2 = \\frac{\\sum_{i=1}^n p_{ji} \\cdot (x_i - \\mu_j)^2}{n_j}\n",
    "       $\n",
    "     - **Updated Mixture Weights** ($ p_j $):  \n",
    "       $\n",
    "       p_j = \\frac{n_j}{n}\n",
    "       $\n",
    "\n",
    "4. **Repeat** the E-Step and M-Step until convergence (parameters stabilize).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Soft vs. Hard Clustering**\n",
    "- **Hard Clustering** (e.g., K-means):  \n",
    "  - A point belongs to only one cluster (0 or 1 assignment).\n",
    "- **Soft Clustering** (Mixture Models):  \n",
    "  - A point belongs to all clusters with certain probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Key Challenges of the EM Algorithm**\n",
    "1. **Local Convergence**:  \n",
    "   - EM is guaranteed to converge **locally** but not globally optimal solutions.  \n",
    "   - Different random initializations may lead to different results.  \n",
    "   - Solution: Use K-means to initialize parameters or try multiple runs.\n",
    "\n",
    "2. **Complexity**:  \n",
    "   - Involves iterative computation of probabilities and parameter updates.\n",
    "\n",
    "3. **Non-Trivial Log-Likelihood**:  \n",
    "   - The log of sums (used in the likelihood) makes direct maximization difficult, requiring the iterative EM framework.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Applications of Mixture Models**\n",
    "- Data segmentation and clustering.  \n",
    "- Modeling **soft memberships** in data points (e.g., overlapping clusters).  \n",
    "- Used in image processing, anomaly detection, and natural language processing.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Takeaways**\n",
    "- Mixture models combine multiple Gaussian distributions to model complex data.  \n",
    "- The **Expectation-Maximization (EM) algorithm** iteratively estimates cluster probabilities and updates parameters to maximize the likelihood.  \n",
    "- EM provides a **soft clustering** approach, allowing points to belong to multiple clusters probabilistically.  \n",
    "- Initialization and convergence properties are critical for EM's success.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
