{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 5. Linear Regression**\n",
    "\n",
    "# **Summary of Linear Classifiers and the Perceptron Algorithm**\n",
    "\n",
    "## **1. Introduction to Linear Classifiers**\n",
    "- **Goal**: Learn a function that maps input feature vectors $ x $ to binary labels $ y \\in \\{-1, +1\\} $.\n",
    "- **Definition**:\n",
    "   - A **linear classifier** divides the feature space into two regions using a hyperplane:\n",
    "     $\n",
    "     h(x) = \\text{sign}(\\theta^\\top x + \\theta_0)\n",
    "     $\n",
    "     - $ \\theta $: Weight vector (determines the direction of separation).\n",
    "     - $ \\theta_0 $: Offset (shifts the hyperplane).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Linear Classifier Properties**\n",
    "- **Linear Separability**:\n",
    "   - A dataset is linearly separable if a hyperplane exists that separates positive and negative labels.\n",
    "- **Training and Test Error**:\n",
    "   - **Training Error**: Fraction of misclassified training samples.\n",
    "   - **Test Error**: Fraction of misclassified unseen samples.\n",
    "   - **Goal**: Minimize test error by choosing a classifier that generalizes well.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Perceptron Algorithm**\n",
    "- **Objective**: Find a linear classifier that minimizes the training error, given a linearly separable dataset.\n",
    "- **Algorithm Steps**:\n",
    "  1. **Initialization**:\n",
    "     - Set $ \\theta = 0 $, $ \\theta_0 = 0 $ (optional offset).\n",
    "  2. **Iterate Through Training Examples**:\n",
    "     - For a misclassified example $ (x_i, y_i) $, update:\n",
    "       $\n",
    "       \\theta \\leftarrow \\theta + y_i x_i\n",
    "       $\n",
    "       $\n",
    "       \\theta_0 \\leftarrow \\theta_0 + y_i\n",
    "       $\n",
    "  3. **Repeat Until Convergence**:\n",
    "     - Iterate through the dataset multiple times until no misclassifications occur&#8203;:contentReference.\n",
    "\n",
    "- **Key Insight**:\n",
    "   - Each update adjusts the hyperplane toward correctly classifying the current example.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Perceptron Algorithm Analysis**\n",
    "- **Correctness**:\n",
    "   - If the data is linearly separable, the perceptron algorithm converges to a solution in a finite number of updates.\n",
    "- **Limitations**:\n",
    "   - The algorithm cannot handle datasets that are not linearly separable.\n",
    "   - There are infinitely many valid solutions, and the algorithm does not optimize for margins (distance between points and the hyperplane).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Loss Functions for Linear Classifiers**\n",
    "- **Training Error**:\n",
    "   - Measures whether predictions match labels but does not provide a gradient for optimization.\n",
    "- **Perceptron Loss**:\n",
    "   - Penalizes misclassified points:\n",
    "     $\n",
    "     \\text{Loss} = \\max(0, 1 - y (\\theta^\\top x + \\theta_0))\n",
    "     $\n",
    "- **Hinge Loss** (used in SVMs):\n",
    "   - Encourages a margin of at least 1 for correctly classified points:\n",
    "     $\n",
    "     \\text{Loss} = \\max(0, 1 - y (\\theta^\\top x + \\theta_0))\n",
    "     $\n",
    "   - Differs from perceptron loss by capping the penalty for well-classified points.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Advantages and Limitations of Linear Classifiers**\n",
    "### **Advantages**:\n",
    "1. **Simplicity**: Computationally efficient and easy to implement.\n",
    "2. **Interpretability**: Decisions are based on a linear combination of features.\n",
    "3. **Scalability**: Works well with high-dimensional data.\n",
    "\n",
    "### **Limitations**:\n",
    "1. **Limited Flexibility**:\n",
    "   - Linear classifiers fail for datasets that are not linearly separable.\n",
    "2. **Sensitivity to Outliers**:\n",
    "   - Outliers can significantly affect the learned hyperplane.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Applications of Linear Classifiers**\n",
    "- Widely used for binary classification tasks such as:\n",
    "  - Spam detection.\n",
    "  - Sentiment analysis.\n",
    "  - Disease diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Key Takeaways**\n",
    "- Linear classifiers divide the feature space using a hyperplane and are suitable for linearly separable data.\n",
    "- The perceptron algorithm is a simple method for finding a linear classifier but cannot handle non-linear problems or optimize for margins.\n",
    "- Advanced methods like SVMs (support vector machines) extend linear classifiers with margin optimization and kernel techniques for non-linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 3 Hinge loss, Margin boundaries and Regularization 10 of 12 possible points**\n",
    "\n",
    "### **Summary of Large Margin Classification and Hinge Loss**\n",
    "\n",
    "## **1. Introduction to Large Margin Classification**\n",
    "- **Goal**: Find a linear classifier that maximizes the **margin** (distance between decision boundary and closest points).\n",
    "- Large margin classifiers are more **robust to noise** in the training data compared to classifiers that tightly fit the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Margin Boundaries**\n",
    "- **Decision Boundary**: Defined as the set of points satisfying:\n",
    "   $\n",
    "   \\theta^\\top x + \\theta_0 = 0\n",
    "   $\n",
    "- **Margin Boundaries**: Parallel lines equidistant from the decision boundary:\n",
    "   - Positive margin boundary: $ \\theta^\\top x + \\theta_0 = 1 $\n",
    "   - Negative margin boundary: $ \\theta^\\top x + \\theta_0 = -1 $\n",
    "- Distance between margin boundaries is inversely proportional to the norm of $ \\theta $:\n",
    "   $\n",
    "   \\text{Margin} = \\frac{2}{\\|\\theta\\|}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Objective for Large Margin Classification**\n",
    "- The goal is to maximize the margin while ensuring correct classification of training data:\n",
    "   - Regularization term: Encourages a large margin by minimizing $ \\|\\theta\\|^2 $.\n",
    "   - Loss term: Penalizes misclassified or boundary-violating points.\n",
    "\n",
    "### **Hinge Loss**:\n",
    "- Hinge loss penalizes points inside or on the wrong side of the margin boundary:\n",
    "   $\n",
    "   \\text{Hinge Loss} = \\max(0, 1 - y (\\theta^\\top x + \\theta_0))\n",
    "   $\n",
    "- **Interpretation**:\n",
    "   - Loss is $ 0 $ when $ y (\\theta^\\top x + \\theta_0) \\geq 1 $ (correctly classified with margin).\n",
    "   - Increases linearly when points violate the margin.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Regularization and Trade-off**\n",
    "- **Objective Function**: Balances loss and margin regularization:\n",
    "   $\n",
    "   J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i (\\theta^\\top x_i + \\theta_0)) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "   $\n",
    "   - First term: Average hinge loss over all training examples.\n",
    "   - Second term: Regularization penalty (controls margin size).\n",
    "\n",
    "- **Regularization Parameter ($ \\lambda $)**:\n",
    "   - Large $ \\lambda $: Favors large margins but allows some training loss.\n",
    "   - Small $ \\lambda $: Prioritizes minimizing training loss, possibly at the cost of a small margin.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Optimization for Large Margin Classifiers**\n",
    "- **Gradient-Based Updates**:\n",
    "   - Minimize the objective function iteratively using gradient descent.\n",
    "   - Update rule for $ \\theta $:\n",
    "     $\n",
    "     \\theta \\leftarrow \\theta + \\eta \\cdot y \\cdot x \\quad \\text{if } y (\\theta^\\top x + \\theta_0) < 1\n",
    "     $\n",
    "   - $ \\eta $: Learning rate that controls step size.\n",
    "\n",
    "- **Steps**:\n",
    "   1. Initialize $ \\theta $ and $ \\theta_0 $.\n",
    "   2. For each misclassified point, update $ \\theta $ to reduce hinge loss.\n",
    "   3. Regularize by scaling $ \\theta $ to maintain margin.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Geometric Interpretation**\n",
    "- **Signed Distance**:\n",
    "   - Measures how far a point lies from the decision boundary.\n",
    "   - Positive distance: Correct side of the margin.\n",
    "   - Negative distance: Violates the margin.\n",
    "\n",
    "- **Robustness**:\n",
    "   - Large margins ensure better generalization to test data and robustness to small perturbations.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Key Takeaways**\n",
    "- Large margin classification improves robustness by maximizing the distance between decision boundaries and training points.\n",
    "- **Hinge loss** quantifies how much a point violates the margin boundary, while regularization controls the size of the margin.\n",
    "- The optimization objective balances hinge loss and regularization to find the best linear classifier.\n",
    "- Gradient-based methods iteratively minimize the loss function, ensuring convergence to a solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lecture 4. Linear Classification and Generalization**\n",
    "\n",
    "### **Summary of Regularization, Large Margins, and Stochastic Optimization**\n",
    "\n",
    "## **1. Introduction to Regularization and Large Margins**\n",
    "- **Objective**: Balance goodness-of-fit to data and intrinsic plausibility of a model using regularization.\n",
    "- **Key Idea**:\n",
    "   - Regularization discourages overreliance on features and favors simpler models by penalizing large weights (e.g., L2 norm: $ \\|\\theta\\|^2 $).\n",
    "   - In classification, maximizing the margin between classes improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Regularization in Large Margin Classification**\n",
    "- **Hinge Loss**:\n",
    "   - Penalizes examples within the margin or misclassified:\n",
    "     $\n",
    "     \\text{Loss} = \\max(0, 1 - y (\\theta^\\top x + \\theta_0))\n",
    "     $\n",
    "   - Zero loss for correctly classified points outside the margin.\n",
    "\n",
    "- **Objective Function**:\n",
    "   - Combines hinge loss with regularization:\n",
    "     $\n",
    "     J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i (\\theta^\\top x_i + \\theta_0)) + \\frac{\\lambda}{2} \\|\\theta\\|^2\n",
    "     $\n",
    "   - $ \\lambda $: Regularization parameter controlling the trade-off between margin size and fit to training data.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Effects of Regularization Parameter ($ \\lambda $)**\n",
    "- **High $ \\lambda $**:\n",
    "   - Emphasizes larger margins.\n",
    "   - Allows for some training loss to prioritize generalization.\n",
    "- **Low $ \\lambda $**:\n",
    "   - Reduces training loss but risks overfitting by shrinking the margin.\n",
    "\n",
    "- **Training vs. Test Loss**:\n",
    "   - **U-shaped Test Loss Curve**:\n",
    "     - **Underfitting**: Too much regularization (high $ \\lambda $) leads to poor fit.\n",
    "     - **Overfitting**: Too little regularization (low $ \\lambda $) fits training data too closely.\n",
    "     - Optimal $ \\lambda $ balances these effects to minimize test loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Stochastic Gradient Descent (SGD)**\n",
    "- **Why Use SGD**:\n",
    "   - Efficient for large datasets by updating weights using a single or small batch of training examples.\n",
    "\n",
    "- **Update Rule**:\n",
    "   $\n",
    "   \\theta \\leftarrow \\theta - \\eta (\\nabla \\text{Loss} + \\lambda \\theta)\n",
    "   $\n",
    "   - $ \\eta $: Learning rate.\n",
    "   - Includes regularization gradient ($ \\lambda \\theta $) to shrink weights.\n",
    "\n",
    "- **Steps**:\n",
    "   1. Sample a training example $ (x_i, y_i) $.\n",
    "   2. Compute gradient for hinge loss:\n",
    "      $\n",
    "      \\nabla = \n",
    "      \\begin{cases} \n",
    "      0 & \\text{if } y (\\theta^\\top x_i + \\theta_0) \\geq 1 \\\\\n",
    "      -y x_i & \\text{if } y (\\theta^\\top x_i + \\theta_0) < 1 \n",
    "      \\end{cases}\n",
    "      $\n",
    "   3. Update $ \\theta $.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Quadratic Programming for Support Vector Machines (SVMs)**\n",
    "- **Formulation**:\n",
    "   - Exact solutions for maximum margin problems can be obtained using quadratic programming.\n",
    "   - In separable cases, constraints enforce all points lie outside the margin.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Takeaways**\n",
    "- **Regularization**:\n",
    "   - Reduces overfitting by controlling model complexity (e.g., penalizing large weights).\n",
    "   - The regularization parameter ($ \\lambda $) balances margin size and training loss.\n",
    "- **Large Margins**:\n",
    "   - Improve robustness to noise and enhance generalization.\n",
    "   - Margins are controlled by the regularization term $ \\|\\theta\\|^2 $.\n",
    "- **Optimization**:\n",
    "   - Stochastic gradient descent efficiently optimizes large datasets by updating weights incrementally.\n",
    "   - Quadratic programming offers exact solutions for simpler cases but is less scalable than SGD.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
